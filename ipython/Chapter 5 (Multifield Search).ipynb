{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code (Listings 3 & 4) 5.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Some utilities for flattening the explain into something a bit more\n",
    "# readable. Pass Explain JSON, get something readable (ironically this is what Solr's default output is :-p)\n",
    "def flatten(l):\n",
    "    [item for sublist in l for item in sublist]\n",
    "\n",
    "def simplerExplain(explainJson, depth=0):\n",
    "    result = \" \" * (depth * 2) + \"%s, %s\\n\" % (explainJson['value'], explainJson['description'])\n",
    "    #print json.dumps(explainJson, indent=True)\n",
    "    if 'details' in explainJson:\n",
    "        for detail in explainJson['details']:\n",
    "            result += simplerExplain(detail, depth=depth+1)\n",
    "    return result\n",
    "\n",
    "\n",
    "# To speed up the pace of development, we really need to focus more heavily on the analysis and query\n",
    "# settings of the search engine, rather than fidly bits of the http interface.\n",
    "#\n",
    "# To that end, we're going to collapse some of the code you were introduced to in chapter 3 into more general functions,\n",
    "# so we can reuse them. Largely, this is the exact same code you saw in chapter 3 some more generality.\n",
    "\n",
    "## Analyze\n",
    "## The analyze function is a helper for accessing the _analyze endpoint like we did in chapter 3. Recall,\n",
    "## given a field or analyzer, passing some text to _analyze will return the token stream that results from\n",
    "## that analyzer. This token stream, if you recall, shows us exactly how the search engine translate text\n",
    "## into individual tokens to be consumed by the underlying data structures. When we debug analysis, we see\n",
    "## matches we need to expect.\n",
    "def analyze(text, field=None, analyzer=None):\n",
    "    whatToAnalyze = ''\n",
    "    if field is not None:\n",
    "        whatToAnalyze = \"field=%s\" % field\n",
    "    elif analyzer is not None:\n",
    "        whatToAnalyze = \"analyzer=%s\" % analyzer\n",
    "    resp = requests.get(\"http://localhost:9200/tmdb/_analyze?%s&format=yaml\", whatToAnalyze, \n",
    "                        json={'field': field, 'analyzer': analyzer, 'text':text})\n",
    "    print(resp.text)\n",
    "    \n",
    "## Search\n",
    "## Next we need to wrap up our execution of query DSL queries. The function 'search' will execute the passed query DSL\n",
    "## query and display the results. \n",
    "## If a scoring explain is associated with the results, then it also gets displayed,\n",
    "## We'll also be sure to dump the query DSL\n",
    "def search(query, verbose=False):\n",
    "    url = 'http://localhost:9200/tmdb/movie/_search'\n",
    "    httpResp = requests.get(url, json=query)\n",
    "    if httpResp.status_code != 200:\n",
    "        print(\"Search Failed <%s>\" % httpResp.status_code)\n",
    "        print(\"%s\" % httpResp.text)\n",
    "    searchHits = json.loads(httpResp.text)['hits']\n",
    "    print(\"Num\\tRelevance Score\\t\\tMovie Title\")\n",
    "    for idx, hit in enumerate(searchHits['hits']):\n",
    "            castNames = []            \n",
    "            castCharacters = []                        \n",
    "            directorNames = []\n",
    "            for cast in hit['_source']['cast']:\n",
    "                castNames.append(cast['name'])\n",
    "                castCharacters.append(cast['character'])\n",
    "            for director in hit['_source']['directors']:\n",
    "                directorNames.append(director['name'])\n",
    "            print(\"%s\\t%s\\t\\t%s\" % (idx + 1, hit['_score'], hit['_source']['title']))\n",
    "            if verbose:\n",
    "                print(\"%s\" % hit['_source']['title'])\n",
    "                print(\"%s\" % hit['_source']['tagline'])      \n",
    "                print(\"%s\" % hit['_source']['overview'])\n",
    "                print(\"%s\" % hit['_id'])\n",
    "                print(\"DIRS %s\" % directorNames)\n",
    "                print(\"CAST %s\" % castNames)\n",
    "                print(\"CHAR %s\" % castCharacters)\n",
    "                if '_explanation' in hit:\n",
    "                    print(\"%s\" % simplerExplain(hit['_explanation']))\n",
    "                    print(\"*************************************\")\n",
    "    \n",
    "    if verbose:\n",
    "        httpResp = requests.get('http://localhost:9200' + \n",
    "                    '/tmdb/movie/_validate/query?explain',\n",
    "                     json={'query': query['query']})\n",
    "        print(json.loads(httpResp.text))\n",
    "\n",
    "## Reindex\n",
    "## Reindex takes analyzer and field mappings, recreates the index, and then reindexes\n",
    "## TMDB movies using the _bulk index API. There are other ways for modifying the configuration\n",
    "## of the index besides dropping and restarting, however for convenience and because our data\n",
    "## isn't truly that large, we'll just delete and start from scratch when we need to.\n",
    "def reindex(analysisSettings, mappingSettings=None, movieDict={}):\n",
    "    # Destroy any existing index (equiv to SQL \"drop table\")\n",
    "    resp = requests.delete(\"http://localhost:9200/tmdb\")\n",
    "    print(\"Delete TMDB Index <%s>\" % resp.status_code)\n",
    "    \n",
    "    # Create the index with explicit settings\n",
    "    # We need to explicitely set number of shards to 1 to eliminate the impact of \n",
    "    # distributed IDF on our small collection\n",
    "    # See also \"Relavance is Broken!\"\n",
    "    # http://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"index\": {\n",
    "                \"analysis\" : analysisSettings,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if mappingSettings:\n",
    "        settings['mappings'] = mappingSettings\n",
    "    resp = requests.put(\"http://localhost:9200/tmdb\", json=settings)\n",
    "    print(\"Create TMDB Index <%s>\" % resp.status_code)\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.text)\n",
    "    \n",
    "    # Bulk index title & overview to the movie endpoint\n",
    "    print(\"Indexing %i movies\" % len(movieDict.keys()))\n",
    "    bulkMovies = \"\"\n",
    "    for id, movie in movieDict.items():\n",
    "        addCmd = {\"index\": {\"_index\": \"tmdb\", \"_type\": \"movie\", \"_id\": movie[\"id\"]}}\n",
    "        esDoc  = movie\n",
    "        bulkMovies += json.dumps(addCmd) + \"\\n\" + json.dumps(esDoc) + \"\\n\"\n",
    "    resp = requests.post(\"http://localhost:9200/_bulk\", data=bulkMovies, headers={\"Content-Type\":\"application/x-ndjson\"})\n",
    "    print(\"Bulk Index into TMDB Index <%s>\" % resp.status_code)\n",
    "\n",
    "## Extract\n",
    "## major difference between our use of TMDB here and in chapter 3: pulling more data. Not only do we access the \n",
    "## movie endpoint, we also extract the credits -- pulling in the cast (actors and such) and extracting the director.\n",
    "def extract(movieIds=[], numMovies=10000):\n",
    "    if len(movieIds) == 0:\n",
    "        try:\n",
    "            f = open('tmdb.json')\n",
    "            if f:\n",
    "                return json.loads(f.read());\n",
    "        except IOError:\n",
    "            pass       \n",
    "    return movieDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.2 -- Listing 4, Index to ES, Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movieDict = extract()\n",
    "\n",
    "analysis = {\n",
    "    \"analyzer\" : {\n",
    "      \"default\" : {\n",
    "        \"type\" : \"english\"\n",
    "        }\n",
    "      }\n",
    "   }\n",
    "\n",
    "reindex(analysisSettings=analysis, movieDict=movieDict)\n",
    "\n",
    "usersSearch = 'basketball with cartoon aliens'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title^0.1', 'overview'],\n",
    "         }\n",
    "    },\n",
    "    'size': 5,\n",
    "    'explain': True\n",
    "}\n",
    "search(query)\n",
    "print(\"===============\")\n",
    "search(query, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.4 -- Listing 5 Inspecting Nested Star Trek Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spaceJamId = 2300\n",
    "httpResp = requests.get(\"http://localhost:9200/tmdb/movie/%s\" % spaceJamId)\n",
    "spaceJamDoc = json.loads(httpResp.text)\n",
    "print(json.dumps(spaceJamDoc['_source'], indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.1, Listing 6 Star Trek Query Using Query from Ch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'patrick stewart'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title', 'overview', 'cast.name', 'directors.name'],      \n",
    "            'type': 'best_fields'\n",
    "         }\n",
    "    },\n",
    "    'size': 50,\n",
    "    'explain': True\n",
    "}\n",
    "search(query)\n",
    "print \"===============\"\n",
    "search(query, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.2 -- Listing 7 -- Reducing the Impact of directors.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'patrick stewart'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title', 'overview',\n",
    "                       'cast.name', 'directors.name^0.1'],  #A    \n",
    "         }\n",
    "    },\n",
    "}\n",
    "search(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.3 -- Listings 8&9  – Analysis Extracting English Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysisSettings = {\n",
    "   \"analyzer\" : {\n",
    "      \"default\" : {\n",
    "        \"type\" : \"english\"\n",
    "      },\n",
    "      \"english_bigrams\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"standard\",\n",
    "            \"lowercase\",\n",
    "            \"porter_stem\",\n",
    "            \"bigram_filter\"\n",
    "          ]\n",
    "      }\n",
    "    },\n",
    "  \"filter\": {\n",
    "    \"bigram_filter\": {\n",
    "        \"type\": \"shingle\",\n",
    "        \"max_shingle_size\":2,\n",
    "        \"min_shingle_size\":2,\n",
    "        \"output_unigrams\":\"false\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# From listing 9\n",
    "mappingSettings = {\n",
    "    'movie': {\n",
    "        'properties': {\n",
    "            \"cast\": {\n",
    "               'properties': {\n",
    "                  'name': {\n",
    "                      'type': 'string',\n",
    "                      'analyzer': 'english',\n",
    "                      'fields': {\n",
    "                         \"bigramed\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"analyzer\": \"english_bigrams\"\n",
    "                        }     \n",
    "                      }\n",
    "                   }\n",
    "                   \n",
    "               }\n",
    "            },\n",
    "            \"directors\": {\n",
    "               'properties': {\n",
    "                  'name': {\n",
    "                      'type': 'string',\n",
    "                      'analyzer': 'english',\n",
    "                      'fields': {\n",
    "                         \"bigramed\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"analyzer\": \"english_bigrams\"\n",
    "                        }     \n",
    "                      }\n",
    "                   }\n",
    "                   \n",
    "               }\n",
    "            }            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "reindex(analysisSettings, mappingSettings, movieDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.3 -- Listing 10 -- Searching *.bigramed fields, reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'patrick stewart'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title', 'overview',\n",
    "'cast.name.bigramed', 'directors.name.bigramed'],      \n",
    "         }\n",
    "    },\n",
    "}\n",
    "search(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.4\tLetting Losers Share The Glory (no listing number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'star trek patrick stewart'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title', 'overview', 'cast.name.bigramed^5', 'directors.name.bigramed'],      \n",
    "            'type': 'best_fields',\n",
    "            'tie_breaker': 0.4\n",
    "         }\n",
    "    },\n",
    "    'size': 5,\n",
    "    'explain': True\n",
    "}\n",
    "search(query)\n",
    "print \"===============\"\n",
    "search(query, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.5, Listing 11 Counting Multiple Signals using Most Fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'star trek patrick stewart'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title', 'overview',\n",
    "'cast.name.bigramed', 'directors.name.bigramed'],      \n",
    "            'type': 'most_fields'\n",
    "         }\n",
    "    }\n",
    "}\n",
    "search(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.6, Listing 12\tBoosting in Most-Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'star trek patrick stewart'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title^0.2', 'overview',\n",
    " 'cast.name.bigramed', 'directors.name.bigramed'],      \n",
    "            'type': 'most_fields'\n",
    "         }\n",
    "    },\n",
    "}\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.7\tWhen Additional Matches Don’t Matter (no listing number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usersSearch = 'star trek patrick stewart william shatner'\n",
    "query = {\n",
    "    'query': {\n",
    "        'multi_match': { \n",
    "            'query': usersSearch,  #User's query\n",
    "            'fields': ['title', 'overview', 'cast.name.bigramed', 'directors.name.bigramed'],      \n",
    "            'type': 'most_fields'\n",
    "         }\n",
    "    },\n",
    "    'size': 5,\n",
    "    'explain': True\n",
    "}\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
